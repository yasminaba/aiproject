{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\pyproj\\aiproject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, hamming_loss\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lyrics_emotions_dataset.csv') \n",
    "df.drop(columns=[\"album\", \"year\"], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lyrics(lyrics):\n",
    "    lyrics = lyrics.lower() # convert to lowercase\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics) # remove [chorus], [verse], etc.\n",
    "    lyrics = re.sub(r'\\{.*?\\}', '', lyrics) # remove {chorus}, {verse}, etc.\n",
    "    lyrics = re.sub(r'\\n', ' ', lyrics) # remove newline characters\n",
    "    lyrics = re.sub(r'\\s+', ' ', lyrics) # remove extra whitespace\n",
    "    lyrics = lyrics.strip() # remove leading and trailing whitespace\n",
    "    return lyrics\n",
    "\n",
    "df['lyrics'] = df['lyrics'].apply(preprocess_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "df['labels'] = df['labels'].apply(lambda x: x.split(', ') if isinstance(x, str) else x)\n",
    "\n",
    "labels_binarized = mlb.fit_transform(df['labels'])\n",
    "\n",
    "encoded_df = pd.DataFrame(labels_binarized, columns=mlb.classes_)\n",
    "\n",
    "binary_columns = list(mlb.classes_)\n",
    "df.drop(columns=binary_columns, errors=\"ignore\", inplace=True)\n",
    "\n",
    "df = pd.concat([df, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['lyrics'].tolist()\n",
    "y = labels_binarized\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train, truncation=True, padding=True, return_tensors='pt', max_length=512)\n",
    "X_test_tokenized = tokenizer(X_test, truncation=True, padding=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = {key: val.to(device) for key, val in encodings.items()}\n",
    "        self.labels = labels.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "    \n",
    "train_dataset = LyricsDataset(X_train_tokenized, y_train)\n",
    "test_dataset = LyricsDataset(X_test_tokenized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\pyproj\\aiproject\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_35352\\1550151334.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 2:52:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.636900</td>\n",
       "      <td>0.632044</td>\n",
       "      <td>0.129203</td>\n",
       "      <td>0.348659</td>\n",
       "      <td>58.420800</td>\n",
       "      <td>3.971000</td>\n",
       "      <td>0.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.602785</td>\n",
       "      <td>0.272310</td>\n",
       "      <td>0.323276</td>\n",
       "      <td>53.238000</td>\n",
       "      <td>4.358000</td>\n",
       "      <td>0.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.548400</td>\n",
       "      <td>0.566882</td>\n",
       "      <td>0.410822</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>62.219000</td>\n",
       "      <td>3.729000</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.506400</td>\n",
       "      <td>0.556243</td>\n",
       "      <td>0.447784</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>61.797800</td>\n",
       "      <td>3.754000</td>\n",
       "      <td>0.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.463900</td>\n",
       "      <td>0.547561</td>\n",
       "      <td>0.494364</td>\n",
       "      <td>0.278257</td>\n",
       "      <td>61.330000</td>\n",
       "      <td>3.783000</td>\n",
       "      <td>0.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>0.556367</td>\n",
       "      <td>0.510104</td>\n",
       "      <td>0.280651</td>\n",
       "      <td>92.374700</td>\n",
       "      <td>2.512000</td>\n",
       "      <td>0.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.548154</td>\n",
       "      <td>0.526277</td>\n",
       "      <td>0.273467</td>\n",
       "      <td>52.887000</td>\n",
       "      <td>4.387000</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.550593</td>\n",
       "      <td>0.535853</td>\n",
       "      <td>0.272510</td>\n",
       "      <td>58.012900</td>\n",
       "      <td>3.999000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.551884</td>\n",
       "      <td>0.538978</td>\n",
       "      <td>0.269157</td>\n",
       "      <td>52.950000</td>\n",
       "      <td>4.381000</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.551687</td>\n",
       "      <td>0.545750</td>\n",
       "      <td>0.269636</td>\n",
       "      <td>56.360800</td>\n",
       "      <td>4.116000</td>\n",
       "      <td>0.515000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29/29 01:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5516871809959412, 'eval_f1': 0.5457501490706075, 'eval_hamming_loss': 0.2696360153256705, 'eval_runtime': 64.4222, 'eval_samples_per_second': 3.601, 'eval_steps_per_second': 0.45, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=len(mlb.classes_), \n",
    "    problem_type='multi_label_classification').to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.001,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch', \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    logits = pred.predictions\n",
    "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    return {\n",
    "        'f1': f1_score(labels, preds, average='macro'),\n",
    "        'hamming_loss': hamming_loss(labels, preds)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Power', 'Solemnity', 'Tension')]\n"
     ]
    }
   ],
   "source": [
    "def predict_emotions(lyrics):\n",
    "    lyrics = preprocess_lyrics(lyrics)\n",
    "    input = tokenizer(lyrics, truncation=True, padding=True, return_tensors='pt', max_length = 512)\n",
    "    input = {key: val.to(device) for key, val in input.items()} \n",
    "    logits = model(**input).logits\n",
    "    preds = (torch.sigmoid(logits) > 0.5).int().cpu().numpy()\n",
    "    return mlb.inverse_transform(preds)\n",
    "\n",
    "test_lyrics = \"\"\"Risin' up, back on the street\n",
    "Did my time, took my chances\n",
    "Went the distance, now I'm back on my feet\n",
    "Just a man and his will to survive\n",
    "So many times, it happens too fast\n",
    "You trade your passion for glory\n",
    "Don't lose your grip on the dreams of the past\n",
    "You must fight just to keep them alive\n",
    "It's the eye of the tiger, it's the thrill of the fight\n",
    "Risin' up to the challenge of our rival\n",
    "And the last known survivor stalks his prey in the night\n",
    "And he's watching us all with the eye of the tiger\n",
    "Face to face, out on the heat\n",
    "Hangin' tough, stayin' hungry\n",
    "They stack the odds still we take to the street\n",
    "For the kill, with the skill to survive\n",
    "It's the eye of the tiger, it's the thrill of the fight\n",
    "Risin' up to the challenge of our rival\n",
    "And the last known survivor stalks his prey in the night\n",
    "And he's watching us all with the eye of the tiger\n",
    "Risin' up, straight to the top\n",
    "Had the guts, got the glory\n",
    "Went the distance, now I'm not gonna stop\n",
    "Just a man and his will to survive\n",
    "It's the eye of the tiger, it's the thrill of the fight\n",
    "Risin' up to the challenge of our rival\n",
    "And the last known survivor stalks his prey in the night\n",
    "And he's watching us all with the eye of the tiger\n",
    "The eye of the tiger\n",
    "The eye of the tiger\n",
    "The eye of the tiger\n",
    "The eye of the tiger\"\"\"\n",
    "predicted_emotions = predict_emotions(test_lyrics)\n",
    "print(predicted_emotions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
